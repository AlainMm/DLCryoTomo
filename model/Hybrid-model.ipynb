{"cells":[{"cell_type":"markdown","metadata":{"id":"H4iZVkx4dg8A"},"source":["**Hybrid neural network model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c1LOiRNfK6Vx"},"outputs":[],"source":["# Install the 'jupyterthemes' package using pip\n","# This package allows you to customize the appearance of Jupyter Notebooks\n","!pip install jupyterthemes"]},{"cell_type":"markdown","source":["**Import essential libraries and modules**"],"metadata":{"id":"Edl3HioJ5Isn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jW-d8BPWnpIS"},"outputs":[],"source":["import os  # Provides functions to interact with the operating system (e.g., file paths)\n","import torch  # PyTorch main library for building and training neural networks\n","import PIL  # Python Imaging Library (used for image processing)\n","import torchvision  # Library for image datasets, models, and transformations\n","import random  # For generating random numbers (useful for data shuffling, augmentation, etc.)\n","import matplotlib.pyplot as plt  # For plotting images, graphs, etc.\n","import numpy as np  # For numerical operations and handling arrays\n","\n","# Importing optimization tools from PyTorch\n","import torch.optim as optim\n","\n","# Import image transformation utilities from torchvision\n","import torchvision.transforms as transforms\n","\n","from PIL import Image  # For opening and manipulating image files\n","\n","# Import neural network modules\n","from torch import nn, optim  # nn = neural network layers; optim = optimization algorithms\n","from torch.nn import functional as F  # Functional API for activation functions, loss, etc.\n","\n","# More transformation tools from torchvision (aliased as T)\n","from torchvision import transforms as T\n","from torchvision import transforms\n","\n","# Tools for data loading and dataset splitting\n","from torch.utils.data import DataLoader, Dataset, random_split\n","\n","# Hugging Face Transformers library for using SegFormer model (semantic segmentation)\n","from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n","\n","# Import theme settings for Jupyter Notebook\n","from jupyterthemes import jtplot\n","jtplot.style()  # Apply the Jupyter notebook plot style to match the selected theme"]},{"cell_type":"markdown","metadata":{"id":"KW-eFi55dqu6"},"source":["**Check if a GPU (CUDA) is available.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcBZiC9ypxsy"},"outputs":[],"source":["# If available, use the GPU; otherwise, fall back to the CPU.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Print the selected device (either 'cuda' or 'cpu')\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"J_wKooS6d3T5"},"source":["**Import the module to access Google Drive in Google Colab**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCjGlVhhrARn"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive to access its files from the Colab environment\n","# This will prompt the user to authorize access\n","drive.mount('/content/images')\n","\n","# Set the path to a specific folder in your Google Drive\n","# Replace 'MyDrive/' with the actual path where your data is stored if needed\n","PATH = '/content/images/MyDrive/'"]},{"cell_type":"markdown","source":["**Input images and corresponding masks for the model**"],"metadata":{"id":"tQZXqxEy6WWD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCDO0nM27J6I"},"outputs":[],"source":["# Set the path to the directory containing input images for the model\n","image_directory = \"/content/images/MyDrive/Colab Notebooks/aug_imghybC\"\n","\n","# Set the path to the directory containing corresponding segmentation masks\n","mask_directory = \"/content/images/MyDrive/aug_maskhybC\""]},{"cell_type":"markdown","source":["**TensorFlow/Keras components**"],"metadata":{"id":"bn9_KKH87IXF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DZQ720J2vV_d"},"outputs":[],"source":["# Import NumPy for numerical operations and array handling\n","import numpy as np\n","\n","# Import TensorFlow (the main deep learning library being used)\n","import tensorflow as tf\n","\n","# Import core Keras layers for building CNN models\n","from tensorflow.keras.layers import (\n","    Input,             # Entry point for a model\n","    Conv2D,            # 2D convolution layer\n","    MaxPooling2D,      # Max pooling layer for downsampling\n","    UpSampling2D,      # Upsampling layer (often used in decoders like U-Net)\n","    Concatenate,       # Combine tensors along a specified axis\n","    Add,               # Element-wise addition\n","    Multiply           # Element-wise multiplication\n",")\n","\n","# Import layers for classification and global feature aggregation\n","from tensorflow.keras.layers import (\n","    GlobalAveragePooling2D,  # Averages the spatial dimensions\n","    GlobalMaxPooling2D,      # Takes the max across spatial dimensions\n","    Dense                    # Fully connected layer\n",")\n","\n","# Import the functional API model class\n","from tensorflow.keras.models import Model\n","\n","# Keras backend, useful for custom functions (like loss or metrics)\n","from tensorflow.keras import backend as K\n","\n","# Optimizer for training (Adam is commonly used for its efficiency)\n","from tensorflow.keras.optimizers import Adam\n","\n","# Import additional layers\n","from tensorflow.keras.layers import BatchNormalization, Concatenate, Dropout\n","\n","# Import callbacks for training:\n","# - EarlyStopping: Stops training when validation performance stops improving\n","# - ReduceLROnPlateau: Reduces learning rate when a metric has stopped improving\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Import a pre-trained ResNet50 model (can be used as a backbone for feature extraction)\n","from tensorflow.keras.applications import ResNet50\n","\n","# Matplotlib for visualizing images and training curves\n","import matplotlib.pyplot as plt\n","\n","# OpenCV (cv2) for advanced image processing tasks (reading, resizing, augmenting, etc.)\n","import cv2\n"]},{"cell_type":"markdown","source":["**Channel-wise attention**"],"metadata":{"id":"qSJYYe2hl4GK"}},{"cell_type":"code","source":["from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Reshape, Multiply\n","\n","def channel_attention(x, ratio=16):\n","    # Channel Attention Mechanism using Squeeze-and-Excitation (SE) block\n","\n","    channel = x.shape[-1]  # Get the number of channels in the input feature map\n","\n","    # Step 1: Squeeze — Global Average Pooling\n","    # Reduces the spatial dimensions (H x W) to a single vector (C,)\n","    se = GlobalAveragePooling2D()(x)\n","\n","    # Step 2: Excitation — Fully connected layers\n","    # First dense layer reduces the number of channels (bottleneck)\n","    se = Dense(channel // ratio, activation='relu')(se)\n","\n","    # Second dense layer restores the original channel size and uses sigmoid activation\n","    se = Dense(channel, activation='sigmoid')(se)\n","\n","    # Step 3: Reshape — Convert to (1, 1, C) so it can be multiplied with the input\n","    se = Reshape((1, 1, channel))(se)\n","\n","    # Step 4: Scale — Multiply the input feature map by the attention weights\n","    return Multiply()([x, se])"],"metadata":{"id":"tqj9UcAXl2_M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Spatial Attention mechanism**"],"metadata":{"id":"1dwVKnPV70AJ"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Layer\n","\n","class SpatialAttention(Layer):\n","    def __init__(self, **kwargs):\n","        super(SpatialAttention, self).__init__(**kwargs)\n","        # Convolution layer with 1 output channel and sigmoid activation\n","        # Kernel size 7x7 to capture spatial context\n","        self.conv = Conv2D(1, (7, 7), padding='same', activation='sigmoid')\n","\n","    def call(self, x):\n","        # Compute average pooling along the channel axis (C)\n","        avg_pool = tf.reduce_mean(x, axis=-1, keepdims=True)\n","\n","        # Compute max pooling along the channel axis (C)\n","        max_pool = tf.reduce_max(x, axis=-1, keepdims=True)\n","\n","        # Concatenate both pooling outputs along the channel dimension\n","        concat = tf.concat([avg_pool, max_pool], axis=-1)\n","\n","        # Apply 2D convolution to generate spatial attention map\n","        attention_map = self.conv(concat)\n","\n","        # Multiply the attention map with the input feature map\n","        return Multiply()([x, attention_map])"],"metadata":{"id":"Ndbgx9VKl-Nt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Feature refinement block**"],"metadata":{"id":"wlPfFPiF8rpC"}},{"cell_type":"code","source":["def conv_block(x, filters, kernel_size=3, padding='same', activation='relu'):\n","    # Apply two Conv2D layers with the specified number of filters and activation\n","    x = Conv2D(filters, kernel_size, padding=padding, activation=activation)(x)\n","    x = Conv2D(filters, kernel_size, padding=padding, activation=activation)(x)\n","\n","    # Apply Channel Attention (Squeeze-and-Excitation)\n","    x = channel_attention(x)\n","\n","    # Apply Spatial Attention\n","    x = SpatialAttention()(x)\n","\n","    return x"],"metadata":{"id":"te4hoZV7l-cr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Encoder block**"],"metadata":{"id":"62Pt6aBA8xSz"}},{"cell_type":"code","source":["def encoder_block(x, filters):\n","    # Apply a convolutional block with attention (Conv + Channel + Spatial Attention)\n","    x = conv_block(x, filters)\n","\n","    # Downsample the feature map using MaxPooling\n","    p = MaxPooling2D(pool_size=(2, 2))(x)\n","\n","    # Return both the feature map (x) and the pooled output (p)\n","    return x, p"],"metadata":{"id":"eyNqV_0Pl-hr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Decoder block**"],"metadata":{"id":"XvbPxhKh9FI9"}},{"cell_type":"code","source":["def decoder_block(x, skip, filters):\n","    # Step 1: Upsample using a transposed convolution\n","    x = tf.keras.layers.Conv2DTranspose(filters, (3, 3), strides=(2, 2), padding='same')(x)\n","\n","    # Step 2: Compute required padding to match dimensions with the skip connection\n","    pad_height = skip.shape[1] - x.shape[1]\n","    pad_width = skip.shape[2] - x.shape[2]\n","\n","    if pad_height > 0 or pad_width > 0:\n","        # Apply zero-padding to align spatial dimensions with the skip connection\n","        x = tf.keras.layers.ZeroPadding2D(padding=((pad_height // 2, pad_height - pad_height // 2),\n","                                                   (pad_width // 2, pad_width - pad_width // 2)))(x)\n","\n","    # Step 3: Concatenate the upsampled tensor with the corresponding encoder feature map (skip connection)\n","    x = tf.keras.layers.Concatenate()([x, skip])\n","\n","    # Step 4: Apply convolutional block with attention mechanisms\n","    x = conv_block(x, filters)\n","\n","    return x"],"metadata":{"id":"f8GIdq5Tl-ng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Atrous Spatial Pyramid Pooling (ASPP) for capturing multi-scale context**"],"metadata":{"id":"5Id2WO9v9juc"}},{"cell_type":"code","source":["def aspp_block(x):\n","\n","    # 1x1 convolution (no dilation) – captures local features\n","    aspp1 = Conv2D(256, 1, padding='same', activation='relu')(x)\n","\n","    # 3x3 convolutions with increasing dilation rates to capture multi-scale features\n","    aspp2 = Conv2D(256, 3, padding='same', dilation_rate=6, activation='relu')(x)\n","    aspp3 = Conv2D(256, 3, padding='same', dilation_rate=12, activation='relu')(x)\n","    aspp4 = Conv2D(256, 3, padding='same', dilation_rate=18, activation='relu')(x)\n","\n","    # Additional 1x1 convolution (optional—can be considered a redundant path or auxiliary signal)\n","    aspp5 = Conv2D(256, 1, padding='same', activation='relu')(x)\n","\n","    # Concatenate all ASPP branches along the channel axis\n","    x = Concatenate()([aspp1, aspp2, aspp3, aspp4, aspp5])\n","\n","    # Reduce channel depth after concatenation with another 1x1 conv\n","    x = Conv2D(256, 1, padding='same', activation='relu')(x)\n","\n","    return x"],"metadata":{"id":"C0eXwaejqcRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Deep encoder-decoder CNN for image segmentation**, with:\n","\n","(a) Attention blocks (channel + spatial)\n","\n","(b) ASPP (Atrous Spatial Pyramid Pooling)\n","\n","(c) LSTM layer in the bottleneck to model long-range dependencies\n","\n","(d) level depth (deeper than standard U-Net)\n","\n","(e) Dropout for regularization"],"metadata":{"id":"0e9d7wK8-OGs"}},{"cell_type":"code","source":["def build_deeper_model_with_lstm(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS, dropout_rate=0.5):\n","\n","    # Define the input shape of the model based on image dimensions and channels.\n","    input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n","    inputs = Input(shape=input_shape)\n","\n","    # Encoder Path (Feature Extraction with Downsampling)\n","    # Apply 6 encoder blocks with increasing filter sizes (from 64 to 2048).\n","\n","    # Each block uses:\n","    # 2 Conv2D layers\n","    # Channel and spatial attention\n","    # MaxPooling\n","    # Dropout for regularization\n","\n","    e1, p1 = encoder_block(inputs, 64)\n","    e1 = Dropout(dropout_rate)(e1)\n","    e2, p2 = encoder_block(p1, 128)\n","    e2 = Dropout(dropout_rate)(e2)\n","    e3, p3 = encoder_block(p2, 256)\n","    e3 = Dropout(dropout_rate)(e3)\n","    e4, p4 = encoder_block(p3, 512)\n","    e4 = Dropout(dropout_rate)(e4)\n","    e5, p5 = encoder_block(p4, 1024)\n","    e5 = Dropout(dropout_rate)(e5)\n","    e6, p6 = encoder_block(p5, 2048)\n","    e6 = Dropout(dropout_rate)(e6)\n","\n","    # Bottleneck + ASPP\n","\n","    # The deepest layer (b1) is processed by a convolutional block.\n","    # Then passed through the ASPP block to extract multi-scale contextual features.\n","    b1 = conv_block(p6, 4096)\n","    b1 = Dropout(dropout_rate)(b1)\n","    b2 = aspp_block(b1)\n","\n","    # LSTM Integration (Temporal/Sequential Context in Spatial Features)\n","\n","    # ASPP output is reshaped into a sequence format for LSTM input: shape → (timesteps, features).\n","    # LSTM captures spatial dependencies along flattened feature sequences.\n","    # The output is reshaped back into spatial dimensions to continue decoding.\n","\n","    lstm_input = tf.keras.layers.Reshape((b2.shape[1] * b2.shape[2], b2.shape[3]))(b2)\n","    lstm_out = tf.keras.layers.LSTM(512, return_sequences=True)(lstm_input)\n","    lstm_out = Dropout(dropout_rate)(lstm_out)\n","    lstm_out = Reshape((IMG_HEIGHT // 64, IMG_WIDTH // 64, 512))(lstm_out)\n","\n","    # Decoder Path (Upsampling + Skip Connections)\n","\n","    # Each decoder block:\n","    # Upsamples using transposed convolution\n","    # Aligns and concatenates with the corresponding encoder output (skip connection)\n","    # Refines with conv + attention block\n","    # Dropout is applied after each decoder stage\n","\n","    d1 = decoder_block(lstm_out, e6, 1024)\n","    d1 = Dropout(dropout_rate)(d1)\n","    d2 = decoder_block(d1, e5, 512)\n","    d2 = Dropout(dropout_rate)(d2)\n","    d3 = decoder_block(d2, e4, 256)\n","    d3 = Dropout(dropout_rate)(d3)\n","    d4 = decoder_block(d3, e3, 128)\n","    d4 = Dropout(dropout_rate)(d4)\n","    d5 = decoder_block(d4, e2, 64)\n","    d5 = Dropout(dropout_rate)(d5)\n","    d6 = decoder_block(d5, e1, 32)\n","    d6 = Dropout(dropout_rate)(d6)\n","\n","    # Output Layer\n","    outputs = Conv2D(1, 1, activation='sigmoid')(d6)\n","\n","    # Model Compilation\n","    model = Model(inputs, outputs)\n","    model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n","    model.summary()\n","    return model"],"metadata":{"id":"U8-K3HTUl-uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Importing libraries and modules**"],"metadata":{"id":"2hWl_grFCkc8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3xciTqcWU0OX"},"outputs":[],"source":["# Import the normalize function from Keras utilities for data normalization\n","from keras.utils import normalize\n","\n","# Import the os module to interact with the operating system (e.g., file paths)\n","import os\n","\n","# Import OpenCV library for image processing tasks\n","import cv2\n","\n","# Import PIL (Pillow) library for image manipulation\n","from PIL import Image\n","\n","# Import NumPy for numerical operations and array handling\n","import numpy as np\n","\n","# Import pyplot module from matplotlib for plotting and visualization\n","from matplotlib import pyplot as plt\n","\n","# Import ThreadPoolExecutor for concurrent execution of code using threads\n","from concurrent.futures import ThreadPoolExecutor"]},{"cell_type":"markdown","source":["**Target size**"],"metadata":{"id":"rZkyHg7aEJkO"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBWrUYOXxtHc"},"outputs":[],"source":["# Define the target size (width and height) for images to be processed\n","SIZE = 224\n","\n","# Initialize an empty list to store image data\n","image_dataset = []\n","\n","# Initialize an empty list to store corresponding mask data\n","mask_dataset = []"]},{"cell_type":"markdown","source":["**Lists of filenames from the image and mask directories**"],"metadata":{"id":"-_8cE4RlE8WN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyUBCA7rzX1G"},"outputs":[],"source":["# Get sorted lists of filenames from the image and mask directories\n","images = sorted(os.listdir(image_directory))\n","masks = sorted(os.listdir(mask_directory))\n","\n","# Ensure the number of images matches the number of masks\n","assert len(images) == len(masks), \"The number of images and masks does not match\"\n","\n","# Define a function to load and preprocess an image-mask pair\n","def process_image_mask(image_name, mask_name):\n","    # Check if both files have the '.tif' extension\n","    if image_name.split('.')[1] == 'tif' and mask_name.split('.')[1] == 'tif':\n","        # Construct full file paths for image and mask\n","        image_path = os.path.join(image_directory, image_name)\n","        mask_path = os.path.join(mask_directory, mask_name)\n","\n","        # Load the image in grayscale mode\n","        image = cv2.imread(image_path, 0)\n","        if image is None:\n","            print(f\"Failed to load image: {image_path}\")\n","            return None, None\n","\n","        # Load the mask in grayscale mode\n","        mask = cv2.imread(mask_path, 0)\n","        if mask is None:\n","            print(f\"Failed to load mask: {mask_path}\")\n","            return None, None\n","\n","        # Resize both image and mask to the target size\n","        image = cv2.resize(image, (SIZE, SIZE))\n","        mask = cv2.resize(mask, (SIZE, SIZE))\n","\n","        # Convert images to numpy arrays and return them\n","        return np.array(image), np.array(mask)\n","\n","    # Return None if file extensions are not '.tif'\n","    return None, None\n","\n","# Initialize empty lists to store processed images and masks\n","image_dataset = []\n","mask_dataset = []\n","\n","# Use ThreadPoolExecutor to process image-mask pairs concurrently\n","with ThreadPoolExecutor() as executor:\n","    # Map the processing function to each pair of filenames\n","    results = list(executor.map(lambda pair: process_image_mask(*pair), zip(images, masks)))\n","\n","# Append successfully processed image-mask pairs to the datasets\n","for image, mask in results:\n","    if image is not None and mask is not None:\n","        image_dataset.append(image)\n","        mask_dataset.append(mask)\n","\n","# Indicate that processing is complete\n","print(\"Processing completed\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfWlNmUcxtQj"},"outputs":[],"source":["# Convert the list of images to a NumPy array, normalize pixel values along axis 1,\n","# and expand dimensions to add a channel dimension at the end (grayscale channel)\n","image_dataset = np.expand_dims(normalize(np.array(image_dataset), axis=1), 3)\n","\n","# Convert the list of masks to a NumPy array, expand dimensions to add a channel dimension,\n","# and normalize mask pixel values to the range [0, 1] by dividing by 255\n","mask_dataset = np.expand_dims(np.array(mask_dataset), 3) / 255."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1_uf3WfYxtTp"},"outputs":[],"source":["# Import train_test_split function from scikit-learn to split datasets\n","from sklearn.model_selection import train_test_split\n","\n","# Split the image and mask datasets into training and testing sets\n","# 20% of the data is reserved for testing; random_state ensures reproducibility\n","X_train, X_test, y_train, y_test = train_test_split(\n","    image_dataset, mask_dataset, test_size=0.2, random_state=0\n",")\n","\n","# Further split the training set into a smaller training subset and a quick test subset\n","# This can be used for quicker experiments or validation during development\n","X_train_quick_test, X_test_quick_test, y_train_quick_test, y_test_quick_test = train_test_split(\n","    X_train, y_train, test_size=0.2, random_state=0\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hINm_TCCxtYk"},"outputs":[],"source":["# Import required modules for randomness and numerical operations\n","import random\n","import numpy as np\n","\n","# Randomly select an index from the quick test training set\n","image_number = random.randint(0, len(X_train_quick_test))\n","\n","# Set the figure size for plotting\n","plt.figure(figsize=(12, 6))\n","\n","# Display the input image in the first subplot\n","plt.subplot(121)\n","plt.imshow(np.reshape(X_train_quick_test[image_number], (224, 224)), cmap='gray')\n","plt.title(\"Input Image\")\n","\n","# Display the corresponding mask in the second subplot\n","plt.subplot(122)\n","plt.imshow(np.reshape(y_train_quick_test[image_number], (224, 224)), cmap='gray')\n","plt.title(\"Mask\")\n","\n","# Show the plots\n","plt.show()"]},{"cell_type":"markdown","source":["**ImageDataGenerator for performing data augmentation**"],"metadata":{"id":"zTc6bb5fGyh0"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOoTIhanlLb7"},"outputs":[],"source":["# Import ImageDataGenerator for performing data augmentation\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# Define the data augmentation configuration\n","datagen = ImageDataGenerator(\n","    rotation_range=20,           # Randomly rotate images in the range (degrees)\n","    width_shift_range=0.2,       # Randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.2,      # Randomly shift images vertically (fraction of total height)\n","    shear_range=0.2,             # Shear angle in counter-clockwise direction (in degrees)\n","    zoom_range=0.2,              # Random zoom within the specified range\n","    horizontal_flip=True,        # Randomly flip images horizontally\n","    fill_mode='nearest'          # Strategy for filling in newly created pixels after rotation or shifting\n",")"]},{"cell_type":"markdown","source":["**Configured hybrid model**"],"metadata":{"id":"OsTTrhzSG64Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZiYAcoJx8fE"},"outputs":[],"source":["# Extract the input image dimensions and number of channels from the dataset\n","IMG_HEIGHT = image_dataset.shape[1]\n","IMG_WIDTH  = image_dataset.shape[2]\n","IMG_CHANNELS = image_dataset.shape[3]\n","\n","# Define a function that returns a model configured with the appropriate input shape\n","# Assumes that 'build_deeper_model_with_lstm' is a custom model-building function defined elsewhere\n","def get_jacard_model():\n","    return build_deeper_model_with_lstm(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n","\n","# Instantiate the model using the defined function\n","model_jacard = get_jacard_model()"]},{"cell_type":"markdown","source":["**Callbacks**"],"metadata":{"id":"XHaqw4D_HzHk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqDHPiENmC5Q"},"outputs":[],"source":["# Import training callbacks from Keras\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n","\n","# Define early stopping to prevent overfitting\n","# Training will stop if the validation loss does not improve for 3 consecutive epochs\n","# The model will restore the weights from the epoch with the best validation loss\n","early_stopping = EarlyStopping(\n","    monitor='val_loss',           # Metric to monitor\n","    patience=3,                   # Number of epochs with no improvement before stopping\n","    restore_best_weights=True     # Restore model weights from the epoch with the best value\n",")\n","\n","# Define learning rate reduction strategy\n","# Reduces the learning rate by a factor of 0.5 if the validation loss does not improve for 2 epochs\n","# Ensures that the learning rate never drops below 1e-6\n","reduce_lr = ReduceLROnPlateau(\n","    monitor='val_loss',           # Metric to monitor\n","    factor=0.5,                   # Factor by which the learning rate will be reduced\n","    patience=2,                   # Number of epochs with no improvement before reducing LR\n","    min_lr=1e-6                   # Minimum learning rate\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f1UMbipwmDDL"},"outputs":[],"source":["# Re-instantiate the model by calling the model-building function\n","# This is useful if you want to reset the model (e.g., before retraining)\n","model_jacard = get_jacard_model()"]},{"cell_type":"markdown","source":["**Pre-trained weights**"],"metadata":{"id":"X_mjLPaIIb23"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCscbZlCqxvx"},"outputs":[],"source":["# Load pre-trained weights into the model from a .h5 file\n","# This allows you to resume training or perform evaluation/inference using saved model parameters\n","model_jacard.load_weights('/content/images/MyDrive/CNN-hybrid_weightsCont5.h5')"]},{"cell_type":"markdown","source":["**Training with Data generator**"],"metadata":{"id":"qIBPLe2pI2PN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXgxa68svW2Y"},"outputs":[],"source":["# Train the model using augmented data from the training subset\n","history_jacard = model_jacard.fit(\n","    datagen.flow(X_train_quick_test, y_train_quick_test, batch_size=16),  # Data generator with augmentation\n","    verbose=1,                                                            # Print detailed training progress\n","    epochs=20,                                                            # Train for 20 epochs\n","    validation_data=(X_test, y_test),                                     # Use full test set for validation\n","    callbacks=[early_stopping, reduce_lr],                                # Apply early stopping and learning rate reduction\n","    shuffle=False                                                         # Do not shuffle data between epochs\n",")"]},{"cell_type":"markdown","source":["**Training without Data generator**"],"metadata":{"id":"M4ihUAl8JcKv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"s3uFBfIFmi_9"},"outputs":[],"source":["# Train the model directly on the training subset (without data augmentation)\n","history_jacard = model_jacard.fit(\n","    X_train_quick_test,               # Training images\n","    y_train_quick_test,              # Corresponding training masks\n","    batch_size=16,                   # Number of samples per gradient update\n","    verbose=1,                       # Display detailed logs during training\n","    epochs=10,                       # Train the model for up to 50 epochs\n","    validation_data=(X_test, y_test),# Evaluate the model on the full test set after each epoch\n","    callbacks=[early_stopping, reduce_lr],  # Apply early stopping and learning rate reduction\n","    shuffle=False                    # Do not shuffle the data (important if order matters)\n",")"]},{"cell_type":"markdown","source":["**Save de entire model**"],"metadata":{"id":"hOCr2S5OJ_SD"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ARk1MfFmoi6"},"outputs":[],"source":["# Save the entire model (architecture + weights + optimizer state) to an HDF5 file\n","# This allows the model to be reloaded later using keras.models.load_model()\n","model_jacard.save('/content/images/MyDrive/CNN-hybrid_weightsCont5.h5')"]},{"cell_type":"markdown","source":["**Model's performance**"],"metadata":{"id":"J-ov2BiMKYDY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H40d7zVV-mDq"},"outputs":[],"source":["# Evaluate the model's performance on the test set\n","# Returns loss and any additional metrics (e.g., accuracy, if configured)\n","_, accuracy = model_jacard.evaluate(X_test, y_test)\n","\n","# Print the accuracy as a percentage\n","print(\"Accuracy of Jacard Model is = \", (accuracy * 100.0), \"%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2tE8WIwnUFo"},"outputs":[],"source":["# Extract training and validation loss values from the training history\n","loss = history_jacard.history['loss']\n","val_loss = history_jacard.history['val_loss']\n","\n","# Create a range of epoch numbers for the x-axis\n","epochs = range(1, len(loss) + 1)\n","\n","# Plot training loss in yellow\n","plt.plot(epochs, loss, 'y', label='Training loss')\n","\n","# Plot validation loss in red\n","plt.plot(epochs, val_loss, 'r', label='Validation loss')\n","\n","# Set chart title and axis labels\n","plt.title('Training and Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","\n","# Display the legend and the plot\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["**IoU (Jaccard Index)**"],"metadata":{"id":"Ume8LIQ0LT0m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgBGFiZQnqCN"},"outputs":[],"source":["# Assign the model to a variable for clarity\n","model = model_jacard\n","\n","# Generate predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Apply a threshold to convert predicted probabilities to binary mask (True/False)\n","y_pred_thresholded = y_pred > 0.1\n","\n","# Calculate the intersection (logical AND) between ground truth and prediction masks\n","intersection = np.logical_and(y_test, y_pred_thresholded)\n","\n","# Calculate the union (logical OR) between ground truth and prediction masks\n","union = np.logical_or(y_test, y_pred_thresholded)\n","\n","# Compute the Intersection over Union (IoU) score\n","iou_score = np.sum(intersection) / np.sum(union)\n","\n","# Print the IoU score\n","print(\"IoU score is: \", iou_score)"]},{"cell_type":"markdown","source":["**Dice coefficient**"],"metadata":{"id":"jF-nBgF4StL4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVHe1z_SCrMW"},"outputs":[],"source":["# Import NumPy for numerical operations\n","import numpy as np\n","\n","# Generate predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Apply a threshold to convert predicted probabilities to binary masks\n","y_pred_thresholded = y_pred > 0.1\n","\n","# Calculate the intersection between ground truth and predicted masks\n","intersection = np.logical_and(y_test, y_pred_thresholded)\n","intersection_count = np.sum(intersection)\n","\n","# Calculate the total number of positive pixels in ground truth and prediction\n","y_test_count = np.sum(y_test)\n","y_pred_count = np.sum(y_pred_thresholded)\n","\n","# Compute the Dice coefficient (F1 score for segmentation)\n","dice_coefficient = (2 * intersection_count) / (y_test_count + y_pred_count)\n","\n","# Print the Dice coefficient\n","print(\"Dice coefficient is: \", dice_coefficient)"]},{"cell_type":"markdown","source":["**Jaccard coefficient**"],"metadata":{"id":"7g1lxFoAT8nr"}},{"cell_type":"code","source":["# Generate predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Apply threshold to convert predicted probabilities into binary masks\n","y_pred_thresholded = y_pred > 0.1\n","\n","# Compute intersection: pixels correctly predicted as foreground\n","intersection = np.logical_and(y_test, y_pred_thresholded)\n","\n","# Compute union: all pixels that are foreground in either ground truth or prediction\n","union = np.logical_or(y_test, y_pred_thresholded)\n","\n","# Calculate the Jaccard coefficient (Intersection over Union)\n","jaccard_score = np.sum(intersection) / np.sum(union)\n","\n","# Print the Jaccard coefficient\n","print(\"Jaccard coefficient is: \", jaccard_score)"],"metadata":{"id":"CC6ewjlDTAv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Precision, Recall, F1-score**"],"metadata":{"id":"SFh-wCjVUPZ-"}},{"cell_type":"code","source":["# Import precision, recall, and F1-score metrics from scikit-learn\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","\n","# Generate predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Threshold the predicted probabilities to create binary masks\n","y_pred_thresholded = y_pred > 0.1  # Use 0.1 as the classification threshold\n","\n","# Convert ground truth masks to binary format if they aren't already\n","y_test_binary = y_test > 0.1\n","\n","# Flatten the masks to 1D arrays for metric calculation\n","y_true_flat = y_test_binary.flatten()\n","y_pred_flat = y_pred_thresholded.flatten()\n","\n","# Calculate precision: TP / (TP + FP)\n","precision = precision_score(y_true_flat, y_pred_flat)\n","\n","# Calculate recall: TP / (TP + FN)\n","recall = recall_score(y_true_flat, y_pred_flat)\n","\n","# Calculate F1-score: harmonic mean of precision and recall\n","f1 = f1_score(y_true_flat, y_pred_flat)\n","\n","# Display the computed metrics\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1-score:\", f1)"],"metadata":{"id":"ycweqF-4Tg1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Mean Coverage (mCov) score**"],"metadata":{"id":"jw59JwhYUpX7"}},{"cell_type":"code","source":["# Import NumPy for array operations\n","import numpy as np\n","\n","# Generate predictions on the test set\n","y_pred = model.predict(X_test)\n","\n","# Apply threshold to convert predicted probabilities to binary masks\n","y_pred_thresholded = y_pred > 0.1\n","\n","# Convert ground truth masks to binary format\n","y_test_binary = y_test > 0.1\n","\n","# Ensure predictions and ground truth masks have the same shape\n","assert y_pred_thresholded.shape == y_test_binary.shape\n","\n","# Initialize list to store coverage scores for each image\n","num_images = y_test.shape[0]\n","coverage_scores = []\n","\n","# Loop over each image in the test set\n","for i in range(num_images):\n","    pred = y_pred_thresholded[i].astype(bool).flatten()  # Flatten prediction mask\n","    gt = y_test_binary[i].astype(bool).flatten()         # Flatten ground truth mask\n","\n","    # Compute intersection between prediction and ground truth\n","    intersection = np.logical_and(pred, gt)\n","\n","    # Compute the area of the ground truth mask\n","    gt_area = np.sum(gt)\n","\n","    # Skip images with empty ground truth masks (avoid division by zero)\n","    if gt_area == 0:\n","        continue\n","\n","    # Calculate coverage as intersection over ground truth area\n","    coverage = np.sum(intersection) / gt_area\n","    coverage_scores.append(coverage)\n","\n","# Compute mean coverage score across all valid images\n","if coverage_scores:\n","    mCov = np.mean(coverage_scores)\n","else:\n","    mCov = 0.0\n","\n","# Print the mean coverage score\n","print(\"Mean Coverage (mCov) score:\", mCov)"],"metadata":{"id":"IlnKTrE9Trf8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Predictions**"],"metadata":{"id":"wjJXSg4kVE8-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dErUT3IYvW5Q"},"outputs":[],"source":["# Import required libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define a function to visualize predictions vs ground truth\n","def visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5):\n","\n","    # Generate predictions from the model\n","    preds = model.predict(X_test)\n","\n","    # Convert predicted probabilities to binary masks using the given threshold\n","    preds_binary = (preds > threshold).astype(np.uint8)\n","\n","    # Ensure we do not exceed the number of available test images\n","    num_images = min(num_images, len(X_test))\n","\n","    # Randomly select a subset of indices to visualize\n","    random_indices = np.random.choice(len(X_test), num_images, replace=False)\n","\n","    # Iterate over selected images and display input, ground truth, and prediction\n","    for i in random_indices:\n","        plt.figure(figsize=(15, 5))\n","\n","        # Show the original input image\n","        plt.subplot(1, 3, 1)\n","        plt.title('Original Image')\n","        plt.imshow(X_test[i].reshape(224, 224), cmap='gray')  # Assuming grayscale images\n","        plt.axis('off')\n","\n","        # Show the ground truth mask\n","        plt.subplot(1, 3, 2)\n","        plt.title('Ground Truth Mask')\n","        plt.imshow(y_test[i].reshape(224, 224), cmap='gray')  # Assuming grayscale masks\n","        plt.axis('off')\n","\n","        # Show the predicted mask\n","        plt.subplot(1, 3, 3)\n","        plt.title(f'Predicted Mask (Threshold={threshold})')\n","        plt.imshow(preds_binary[i].reshape(224, 224), cmap='viridis')  # Color map can be adjusted\n","        plt.axis('off')\n","\n","        # Display the plots\n","        plt.show()\n","\n","# Call the function to visualize predictions on 5 random test samples\n","visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5)"]},{"cell_type":"markdown","source":["**Predictions of masks with edges**"],"metadata":{"id":"egc2LfbSWtsu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMeSia0e8Vbh"},"outputs":[],"source":["# Import necessary libraries\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.ndimage import sobel\n","\n","# Function to extract and highlight edges from a prediction mask using Sobel filter\n","def highlight_edges(image, threshold=0.1):\n","    # Compute gradients along x and y axes\n","    sobel_x = sobel(image, axis=0)\n","    sobel_y = sobel(image, axis=1)\n","\n","    # Combine gradients to get edge magnitude\n","    edges = np.hypot(sobel_x, sobel_y)\n","\n","    # Normalize edge values to range [0, 255]\n","    edges = np.uint8(255 * edges / np.max(edges))\n","\n","    # Apply threshold to filter out weak edges\n","    edges[edges < threshold * 255] = 0\n","\n","    return edges\n","\n","# Function to visualize a few test predictions with edge enhancement\n","def visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5):\n","    # Generate predictions from the model\n","    preds = model.predict(X_test)\n","\n","    # Randomly select a subset of test images\n","    random_indices = np.random.choice(len(X_test), num_images, replace=False)\n","\n","    # Loop through selected samples and visualize them\n","    for i in random_indices:\n","        plt.figure(figsize=(18, 6))\n","\n","        # Original input image\n","        plt.subplot(1, 3, 1)\n","        plt.title('Original Image', fontsize=16)\n","        original_image = X_test[i].reshape(224, 224)\n","        plt.imshow(original_image, cmap='gray')\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # Ground truth segmentation mask\n","        plt.subplot(1, 3, 2)\n","        plt.title('Ground Truth Mask', fontsize=16)\n","        ground_truth = y_test[i].reshape(224, 224)\n","        plt.imshow(ground_truth, cmap='gray')\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # Predicted mask with edge highlights\n","        plt.subplot(1, 3, 3)\n","        plt.title('Predicted Mask with Edges', fontsize=16)\n","        predicted_mask = preds[i].reshape(224, 224)\n","        predicted_with_edges = highlight_edges(predicted_mask, threshold)\n","        plt.imshow(predicted_with_edges, cmap='inferno')\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # Improve layout and display\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Call the function to visualize 5 test predictions with edge detection\n","visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WaJf0saXB70w"},"outputs":[],"source":["# Import required libraries\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from scipy.ndimage import sobel\n","import random\n","\n","# Function to extract edges using the Sobel operator and apply thresholding\n","def highlight_edges(image, threshold=0.1):\n","    # Compute gradients along the x and y axes\n","    sobel_x = sobel(image, axis=0)\n","    sobel_y = sobel(image, axis=1)\n","\n","    # Combine the gradients to get edge magnitude\n","    edges = np.hypot(sobel_x, sobel_y)\n","\n","    # Normalize edges to 0–255 scale\n","    edges = np.uint8(255 * edges / np.max(edges))\n","\n","    # Apply a threshold to remove weak edges\n","    edges[edges < threshold * 255] = 0\n","\n","    return edges\n","\n","# Function to visualize original image, ground truth, and predicted mask with edge overlay\n","def visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5):\n","    # Generate predictions from the model\n","    preds = model.predict(X_test)\n","\n","    # Randomly select image indices\n","    random_indices = random.sample(range(len(X_test)), num_images)\n","\n","    # Iterate through selected samples\n","    for i in random_indices:\n","        plt.figure(figsize=(18, 6))\n","\n","        # --- Panel 1: Original Image ---\n","        plt.subplot(1, 3, 1)\n","        plt.title('Original Image', fontsize=16)\n","        original_image = X_test[i].reshape(224, 224)\n","        plt.imshow(original_image, cmap='gray')\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # --- Panel 2: Ground Truth Mask ---\n","        plt.subplot(1, 3, 2)\n","        plt.title('Ground Truth Mask', fontsize=16)\n","        ground_truth = y_test[i].reshape(224, 224)\n","        plt.imshow(ground_truth, cmap='gray')\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # --- Panel 3: Overlay Predicted Edges on Original Image ---\n","        plt.subplot(1, 3, 3)\n","        plt.title('Predicted Mask with Edges', fontsize=16)\n","        predicted_mask = preds[i].reshape(224, 224)\n","\n","        # Highlight edges from the predicted mask\n","        predicted_with_edges = highlight_edges(predicted_mask, threshold)\n","\n","        # Overlay the edges on the original image\n","        plt.imshow(original_image, cmap='gray')  # Base grayscale image\n","        plt.imshow(predicted_with_edges, cmap='jet', alpha=0.65)  # Edge heatmap overlay\n","        plt.axis('off')\n","        plt.gca().add_patch(plt.Rectangle((0, 0), 224, 224, linewidth=2, edgecolor='black', facecolor='none'))\n","\n","        # Improve layout\n","        plt.tight_layout()\n","        plt.show()\n","\n","# Call the visualization function\n","visualize_predictions(model, X_test, y_test, threshold=0.1, num_images=5)"]},{"cell_type":"markdown","source":["**Error map**"],"metadata":{"id":"HyygT5__ZKVN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"htU83lZQ3htR"},"outputs":[],"source":["\n","# Import necessary libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Generate predictions from the model\n","preds = model.predict(X_test, batch_size=32)\n","\n","# Apply threshold to convert prediction probabilities into binary masks\n","y_pred_thresholded = preds > 0.1\n","\n","# Select a random image index from the test set\n","test_img_number = random.randint(0, len(X_test) - 1)\n","\n","# Extract the ground truth and predicted masks for the selected image\n","true_mask = y_test[test_img_number, :, :, 0]\n","pred_mask = y_pred_thresholded[test_img_number, :, :, 0]\n","\n","# Calculate the absolute error map (difference between ground truth and prediction)\n","error_map = np.abs(true_mask - pred_mask)\n","\n","# --- Plot original image, ground truth, and predicted mask ---\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(1, 3, 1)\n","plt.title('Original Image')\n","plt.imshow(X_test[test_img_number, :, :, 0], cmap='gray')\n","plt.axis(False)  # Remove axes\n","plt.grid(False)\n","\n","plt.subplot(1, 3, 2)\n","plt.title('Ground Truth Mask')\n","plt.imshow(true_mask, cmap='gray')\n","plt.axis(False)\n","plt.grid(False)\n","\n","plt.subplot(1, 3, 3)\n","plt.title('Predicted Mask')\n","plt.imshow(pred_mask, cmap='gray')\n","plt.axis(False)\n","plt.grid(False)\n","\n","# --- Plot the error map ---\n","plt.figure(figsize=(10, 5))\n","plt.title('Error Map')\n","plt.imshow(error_map, cmap='hot')  # 'hot' colormap highlights error intensities\n","plt.colorbar()\n","plt.axis(False)\n","plt.grid(False)\n","plt.show()"]},{"cell_type":"markdown","source":["**Morphology**"],"metadata":{"id":"N5q0pKTDZg9g"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBOku5FnBBJM"},"outputs":[],"source":["def apply_morphology(prediction, operation='closing', kernel_size=(5, 5)):\n","    \"\"\"\n","    Apply a specified morphological operation to a batch of binary prediction masks.\n","\n","    Parameters:\n","        prediction (numpy.ndarray): Batch of predicted binary masks with shape (N, H, W, 1).\n","        operation (str): Morphological operation to apply. One of:\n","                         'closing', 'opening', 'dilation', 'erosion'.\n","        kernel_size (tuple): Size of the structuring element (default is (5, 5)).\n","\n","    Returns:\n","        numpy.ndarray: Batch of morphologically processed masks with the same shape as input.\n","    \"\"\"\n","    # Define the structuring element (kernel) using a rectangular shape\n","    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_size)\n","\n","    morphed_predictions = []\n","\n","    # Process each image individually\n","    for i in range(prediction.shape[0]):\n","        img = prediction[i, :, :, 0]  # Extract single mask (H, W)\n","\n","        # Apply the selected morphological operation\n","        if operation == 'closing':\n","            morphed_img = cv2.morphologyEx(img.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n","        elif operation == 'opening':\n","            morphed_img = cv2.morphologyEx(img.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n","        elif operation == 'dilation':\n","            morphed_img = cv2.dilate(img.astype(np.uint8), kernel)\n","        elif operation == 'erosion':\n","            morphed_img = cv2.erode(img.astype(np.uint8), kernel)\n","        else:\n","            raise ValueError(\"Invalid operation. Use 'closing', 'opening', 'dilation', or 'erosion'.\")\n","\n","        # Add channel dimension back (H, W, 1)\n","        morphed_predictions.append(morphed_img[..., np.newaxis])\n","\n","    # Stack all processed masks into a batch (N, H, W, 1)\n","    return np.stack(morphed_predictions, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6tkGGKEJBHrK"},"outputs":[],"source":["# Predict masks for the test set using the trained model\n","y_pred = model_jacard.predict(X_test)\n","\n","# Threshold the predictions to get binary masks\n","y_pred_thresholded = (y_pred > 0.1).astype(np.uint8)\n","\n","# Apply morphological closing operation to clean up the predicted masks\n","y_pred_morphed = apply_morphology(y_pred_thresholded, operation='closing')\n","\n","# Calculate the Intersection over Union (IoU) score between the ground truth and morphologically processed predictions\n","intersection = np.logical_and(y_test, y_pred_morphed)\n","union = np.logical_or(y_test, y_pred_morphed)\n","iou_score = np.sum(intersection) / np.sum(union)\n","\n","# Print the IoU score after morphology\n","print(\"IoU score after morphology is: \", iou_score)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfriUYEQB8Pg"},"outputs":[],"source":["def visualize_results(original_image, ground_truth, prediction, morphed_prediction):\n","    \"\"\"\n","    Visualize the original image, ground truth mask, predicted mask (before and after morphology).\n","\n","    Parameters:\n","        original_image (numpy.ndarray): The original input image (H, W).\n","        ground_truth (numpy.ndarray): The ground truth mask (H, W).\n","        prediction (numpy.ndarray): The predicted binary mask without morphology (H, W).\n","        morphed_prediction (numpy.ndarray): Batch of morphologically processed masks (N, H, W, 1).\n","        test_img_number (int): Index of the image in the morphed_prediction batch to visualize.\n","    \"\"\"\n","    plt.figure(figsize=(12, 8))\n","\n","    plt.subplot(221)\n","    plt.title('Original Image')\n","    plt.imshow(original_image, cmap='gray')\n","    plt.axis('off')\n","\n","    plt.subplot(222)\n","    plt.title('Ground Truth Mask')\n","    plt.imshow(ground_truth, cmap='gray')\n","    plt.axis('off')\n","\n","    plt.subplot(223)\n","    plt.title('Prediction without Morphology')\n","    plt.imshow(prediction, cmap='gray')\n","    plt.axis('off')\n","\n","    plt.subplot(224)\n","    plt.title('Prediction with Morphology')\n","    plt.imshow(morphed_prediction[test_img_number, :, :, 0], cmap='gray')\n","    plt.axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkoSmLLICB3T"},"outputs":[],"source":["# Predict segmentation masks on the test set using the trained model\n","y_pred = model_jacard.predict(X_test)\n","\n","# Apply a threshold to convert predicted probabilities into binary masks\n","# Pixels with a prediction probability greater than 0.1 are set to 1, else 0\n","y_pred_thresholded = (y_pred > 0.1).astype(np.uint8)\n","\n","# Apply morphological closing operation to the thresholded predictions\n","# This helps to remove small holes and smooth the mask edges\n","y_pred_morphed = apply_morphology(y_pred_thresholded, operation='closing')\n","\n","# Randomly select an image index from the test set for visualization\n","test_img_number = random.randint(0, len(X_test))\n","\n","# Retrieve the original test image and corresponding ground truth mask\n","test_img = X_test[test_img_number]\n","ground_truth = y_test[test_img_number]\n","\n","# Visualize the original image, ground truth mask, thresholded prediction,\n","# and morphologically processed prediction side-by-side\n","visualize_results(test_img[:, :, 0], ground_truth[:, :, 0],\n","                  y_pred_thresholded[test_img_number], y_pred_morphed)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}